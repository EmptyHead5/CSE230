在这一节中，讨论了通过优化矩阵乘法程序来显著提升性能的不同方法。原始的Python实现虽然编写简单，但在大矩阵计算中效率低下。
例如，在Google Cloud Engine的n1-standard-96服务器上，计算960x960矩阵需要约5分钟，而4096x4096矩阵则需近6小时。

通过将Python代码转换为更接近硬件的C语言代码，性能可以提升约200倍。后续章节介绍了更多优化技术，
例如数据级并行（使用C语言的内在函数实现，性能提高约8倍）、指令级并行（通过循环展开增加性能约2倍）、
内存层次结构优化（通过缓存分块提升大矩阵计算性能约1.5倍）、以及线程级并行（使用OpenMP的并行循环提升12到17倍）。
这些优化使得最终的C代码在不到一秒内完成原本需要6小时的计算，速度提升接近50,000倍。

这部分内容强调了理解硬件如何运作的重要性，以及通过不同层次的优化技术，如何大幅提升程序性能。


for i in xrange(n):
        for j in xrange(n):
            for k in xrange(n):
                C[i][j] += A[i][k] * B[k][j]

这几行代码是通过循环把a和b在范围n内的点积赋值给c
是累计相乘后相加的结果

这个代码在python的环境下运行缓慢
